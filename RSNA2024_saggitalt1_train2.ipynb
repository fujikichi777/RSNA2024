{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYkS4mUW8S_I"
   },
   "source": [
    "# Unetのセグメンテーションモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 16742,
     "status": "ok",
     "timestamp": 1737001549058,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "yuo5CeTu5FKA",
    "outputId": "ab0b22d5-aba9-453d-ccf4-71f4632f7a85"
   },
   "outputs": [],
   "source": [
    "pip install segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 9991,
     "status": "ok",
     "timestamp": 1737001559045,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "yZLvG6OG5WaK",
    "outputId": "fc429183-b2e1-4a5a-e345-4895402c8ea0"
   },
   "outputs": [],
   "source": [
    "pip install pydicom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 4900,
     "status": "ok",
     "timestamp": 1737001563940,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "8YDQKdEQeIhZ",
    "outputId": "955866f2-8d64-46b2-cbab-51b322a13f16"
   },
   "outputs": [],
   "source": [
    "pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 39377,
     "status": "ok",
     "timestamp": 1737001603314,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "8pPjQICr5bES"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from fastai.vision.all import *\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KM4VlcDe5jxq"
   },
   "outputs": [],
   "source": [
    "CV = 5 #画像分類の学習に用いるクロスバリデーションの数\n",
    "SEED = 777 #シード値\n",
    "fold = 1 #検証用データのfold値\n",
    "PATCH_SIZE = 512 #全体の画像の最終的なサイズ\n",
    "patch_size = 64 #セグメンテーションをして切り取った部分の画像のサイズ\n",
    "TH = .5 #全体の画像から病気であると判定した根拠の部分にマスク値を付けたときの閾値\n",
    "SEG_TRAIN = False#セグメンテーションモデルを動かしたいときはTrueにする\n",
    "SEG = {\n",
    "    'BS':16,#セグメンテーションモデルのバッチサイズ\n",
    "    'LR':5e-4,#セグメンテーションモデルの学習率\n",
    "    'EPOCHS':10#セグメンテーションモデルのエポック数\n",
    "}\n",
    "INF = {\n",
    "    'BS':64,#病気を判定するモデルのバッチサイズ\n",
    "    'LR':1e-5,#病気を判定するモデルの学習率\n",
    "    'EPOCHS':10#病気を判定するモデルのエポック数\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 890728,
     "status": "ok",
     "timestamp": 1736909583862,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "BtNaJnd_51Ah",
    "outputId": "4271284c-6b2a-4aae-d425-6466589c0926"
   },
   "outputs": [],
   "source": [
    "!unzip /content/drive/MyDrive/rsna-2024-lumbar-spine-degenerative-classification.zip -d /content #RSNA2024のzipファイルを今のディレクトリに展開する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qayyAclGBfh"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    #結果の再現性を得るために各種シード値を固定するする関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LtSLq2I6QDk"
   },
   "outputs": [],
   "source": [
    "class myUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(myUNet, self).__init__()\n",
    "\n",
    "        self.UNet = smp.Unet(\n",
    "            encoder_name=\"resnet18\",#セグメンテーションモデルのバックボーンはresnet18を使用\n",
    "            classes=5,#5種類のマスクをつける\n",
    "            in_channels=1 #白黒画像なのでチャンネル数は1\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self,X):\n",
    "        x = self.UNet(X)\n",
    "\n",
    "        min_values = x.view(-1,5,PATCH_SIZE*PATCH_SIZE).min(-1)[0].view(-1,5,1,1)\n",
    "        max_values = x.view(-1,5,PATCH_SIZE*PATCH_SIZE).max(-1)[0].view(-1,5,1,1)\n",
    "        x = (x - min_values)/(max_values - min_values)#全体の画像のテンソルをminmaxscalingで正規化。テンソルのブロードキャスティングに注意すること。\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2K6NoRpo6xDE"
   },
   "outputs": [],
   "source": [
    "idx_map = torch.stack([torch.arange(PATCH_SIZE)]*PATCH_SIZE).to(device)\n",
    "idx_map = torch.stack([idx_map,idx_map.T]).view(1,1,2,PATCH_SIZE,PATCH_SIZE)#画像の各地点にx座標の値とy座標の値を格納したものを用意する\n",
    "class myLoss(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            alpha=.5\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def clone(self):\n",
    "        return myLoss(self.alpha)\n",
    "    def forward(\n",
    "            self,\n",
    "            y,# 予測される病気と判定するマスク値（0～1）\n",
    "            t # 病気だと判定した部分の点座標値\n",
    "        ):\n",
    "        mask_pred = y\n",
    "        _,mask_true = t\n",
    "#       理想的な分布を中心（病気だと判定した部分の座標）から1変数の正規分布を３次元空間上で回転させたような分布（共分散行列がσ^2Iの2次元正規分布ともいえる）であると仮定する。\n",
    "        s2 = torch.as_tensor([PATCH_SIZE/8]*5)#正規分布の分散(σ^2)のパラメタを設定(５つのマスクをまとめて)\n",
    "#       正規分布のパラメタを以下のように設定する\n",
    "        A = -1/(2*s2).to(device)#正規分布の式全体の係数の設定\n",
    "        K = 1/torch.sqrt(2*math.pi*s2).to(device)#正規分布のexpの中の係数の設定\n",
    "        mask_pred = mask_pred*K.view(1,5,1,1)#mask_predの最大値がKになるように合わせる。これは理想的な分布の最大値がKのためそこに合わせる。\n",
    "        mask = idx_map - mask_true.view(-1,5,2,1,1)#病気だと判定した座標からのx座標とy座標の距離をmaskに格納\n",
    "        mask = torch.exp((A.view(-1,5,1,1,1)*mask*mask).sum(2))*K.view(-1,5,1,1)#x座標とy座標の距離を使って距離に応じた正規分布の値を作りmaskに格納。\n",
    "#       ロスは理想的な分布とのcosine類似度にする。コサイン類似度は2次元のものをそのまま1次元として扱って求める。\n",
    "        D = 1 - ((mask*mask_pred).sum())**2/((mask*mask).sum()*(mask_pred*mask_pred).sum())\n",
    "\n",
    "        return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1734573583552,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "wBFkPP8kC7Vb",
    "outputId": "ba0b6449-9c35-469f-8f17-f8869ac02d63"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/content/train.csv')#trainを読み込む\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 354,
     "status": "ok",
     "timestamp": 1734502030632,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "oztvBLniC7YL",
    "outputId": "e7114258-9ae6-4410-9011-9a5e4f82ae3d"
   },
   "outputs": [],
   "source": [
    "diagnosis = list(filter(lambda x: x.find('foraminal') > -1, train.columns))\n",
    "train = train[train[diagnosis].isnull().values.sum(1)==0].reset_index(drop=True)\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4e8A_yUDC7at"
   },
   "outputs": [],
   "source": [
    "train2=train[['study_id']+diagnosis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l04swd1eC7dJ"
   },
   "outputs": [],
   "source": [
    "labels = {\n",
    "    'Normal/Mild':0,\n",
    "    'Moderate':1,\n",
    "    'Severe':2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1734502043638,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "1ffOLDV7C7fd",
    "outputId": "7e857417-d38c-4b85-eed0-55751b4d6e24"
   },
   "outputs": [],
   "source": [
    "df_meta_f = pd.read_csv('/content/train_series_descriptions.csv')\n",
    "df_meta_f.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1734502047159,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "1LB-qO7DC7hr",
    "outputId": "94c9201d-8a05-431a-be37-2650fbb82862"
   },
   "outputs": [],
   "source": [
    "df_coor = pd.read_csv('/content/train_label_coordinates.csv')\n",
    "df_coor.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 326,
     "status": "ok",
     "timestamp": 1734502050309,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "2ITySKUgC7kM",
    "outputId": "30bb7b57-d34d-4e1f-9fcf-548ea8dd0237"
   },
   "outputs": [],
   "source": [
    "LF=df_coor[df_coor['condition']=='Left Neural Foraminal Narrowing'][[\n",
    "    'study_id',\n",
    "    'series_id',\n",
    "    'instance_number',\n",
    "    'level',\n",
    "    'x',\n",
    "    'y'\n",
    "]].sort_values([\n",
    "    'study_id',\n",
    "    'series_id',\n",
    "    'level'\n",
    "])[[\n",
    "    'study_id',\n",
    "    'series_id',\n",
    "    'level',\n",
    "    'instance_number',\n",
    "    'x',\n",
    "    'y'\n",
    "]]\n",
    "LF.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dO32UElzD6AJ"
   },
   "outputs": [],
   "source": [
    "LF = LF[[\n",
    "    'study_id',\n",
    "    'series_id',\n",
    "    'instance_number',\n",
    "    'x',\n",
    "    'y'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1734502055064,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "eUh6SicdEAQK",
    "outputId": "280a930d-3cc3-4dc0-8c4e-63ff89f40d94"
   },
   "outputs": [],
   "source": [
    "LF[[\n",
    "    'x_L1L2',\n",
    "    'y_L1L2',\n",
    "    'x_L2L3',\n",
    "    'y_L2L3',\n",
    "    'x_L3L4',\n",
    "    'y_L3L4',\n",
    "    'x_L4L5',\n",
    "    'y_L4L5',\n",
    "    'x_L5S1',\n",
    "    'y_L5S1',\n",
    "]] = np.tile(LF[['x','y']].values.reshape(-1,1,5,2),(1,5,1,1)).reshape(-1,10)\n",
    "LF = LF.drop(columns=['x','y']).drop_duplicates().reset_index(drop=True)\n",
    "LF.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQYtIgMrEEdM"
   },
   "outputs": [],
   "source": [
    "diagnosis = list(filter(lambda x: x.find('left_neural') > -1, train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "executionInfo": {
     "elapsed": 273,
     "status": "ok",
     "timestamp": 1734502060057,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "SuCozTM4ESBB",
    "outputId": "89aab2e4-9908-49ea-aec0-99c575f16904"
   },
   "outputs": [],
   "source": [
    "LF = LF.merge(train[['study_id']+diagnosis], left_on='study_id', right_on='study_id')\n",
    "LF.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1734502063359,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "N1YvW6hgEWHA",
    "outputId": "7981a9a1-3ba1-4413-e2c1-3f46e955d3cf"
   },
   "outputs": [],
   "source": [
    "diagnosis = {x:x[5:] for x in diagnosis}\n",
    "diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1734502066286,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "eRHGwJ-bEaym",
    "outputId": "f2683d12-7c93-4ba1-cdd7-ff21372f7f35"
   },
   "outputs": [],
   "source": [
    "LF = LF.rename(columns=diagnosis)\n",
    "LF.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1734502070004,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "-baw9q3gEc_1",
    "outputId": "d79f0cf2-feeb-423f-a3fd-57a9ea4ecd5c"
   },
   "outputs": [],
   "source": [
    "RF = df_coor[df_coor['condition']=='Right Neural Foraminal Narrowing'][[\n",
    "    'study_id',\n",
    "    'series_id',\n",
    "    'instance_number',\n",
    "    'level',\n",
    "    'x',\n",
    "    'y'\n",
    "]].sort_values([\n",
    "    'study_id',\n",
    "    'series_id',\n",
    "    'level'\n",
    "])[[\n",
    "    'study_id',\n",
    "    'series_id',\n",
    "    'instance_number',\n",
    "    'level',\n",
    "    'x',\n",
    "    'y'\n",
    "]].drop_duplicates()\n",
    "RF.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QTJecVLEhv_"
   },
   "outputs": [],
   "source": [
    "centers = {}\n",
    "for i in range(len(RF)):\n",
    "    row = RF.iloc[i]\n",
    "    centers[row['study_id']]={}\n",
    "for i in range(len(RF)):\n",
    "    row = RF.iloc[i]\n",
    "    centers[row['study_id']][row['series_id']]={'L1/L2':[],'L2/L3':[],'L3/L4':[],'L4/L5':[],'L5/S1':[]}\n",
    "for i in range(len(RF)):\n",
    "    row = RF.iloc[i]\n",
    "    centers[row['study_id']][row['series_id']][row['level']].append([row['x'],row['y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlX-TNP0Ek1v"
   },
   "outputs": [],
   "source": [
    "coordinates = np.zeros((len(RF),10))\n",
    "coordinates[:] = np.nan\n",
    "for i in range(len(RF)):\n",
    "    row = RF.iloc[i]\n",
    "    for level in centers[row['study_id']][row['series_id']]:\n",
    "        if len(centers[row['study_id']][row['series_id']][level]) > 0:\n",
    "            center = np.array(centers[row['study_id']][row['series_id']][level]).mean(0)\n",
    "            coordinates[\n",
    "                i,\n",
    "                {'L1/L2':0, 'L2/L3':2, 'L3/L4':4, 'L4/L5':6, 'L5/S1':8}[level]:{'L1/L2':0, 'L2/L3':2, 'L3/L4':4, 'L4/L5':6, 'L5/S1':8}[level]+2\n",
    "            ] = center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1734502084537,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "Elh2fvZ0Ep5m",
    "outputId": "790b1843-7801-4c6e-a2c3-01a1dc35bc15"
   },
   "outputs": [],
   "source": [
    "RF = RF[[\n",
    "    'study_id',\n",
    "    'series_id',\n",
    "    'instance_number',\n",
    "    'x',\n",
    "    'y'\n",
    "]]\n",
    "RF[[\n",
    "    'x_L1L2',\n",
    "    'y_L1L2',\n",
    "    'x_L2L3',\n",
    "    'y_L2L3',\n",
    "    'x_L3L4',\n",
    "    'y_L3L4',\n",
    "    'x_L4L5',\n",
    "    'y_L4L5',\n",
    "    'x_L5S1',\n",
    "    'y_L5S1',\n",
    "]] = coordinates\n",
    "RF = RF.drop(columns=['x','y']).drop_duplicates().reset_index(drop=True)\n",
    "RF.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "executionInfo": {
     "elapsed": 294,
     "status": "ok",
     "timestamp": 1734502088397,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "B9cozvtjEtYJ",
    "outputId": "268d3292-47c4-427e-8453-2d501ffb2410"
   },
   "outputs": [],
   "source": [
    "RF = RF[RF[[\n",
    "    'x_L1L2',\n",
    "    'y_L1L2',\n",
    "    'x_L2L3',\n",
    "    'y_L2L3',\n",
    "    'x_L3L4',\n",
    "    'y_L3L4',\n",
    "    'x_L4L5',\n",
    "    'y_L4L5',\n",
    "    'x_L5S1',\n",
    "    'y_L5S1',\n",
    "]].isnull().values.sum(1)==0].reset_index(drop=True)\n",
    "RF.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1734502091706,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "eA-tC2vCEyXX",
    "outputId": "dfb7a1ed-b7e3-4fde-ea1e-f71d3ddbe808"
   },
   "outputs": [],
   "source": [
    "diagnosis = list(filter(lambda x: x.find('right_neural_foraminal') > -1, train.columns))\n",
    "RF = RF.merge(train[['study_id']+diagnosis], left_on='study_id', right_on='study_id')\n",
    "RF.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1734502094358,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "Bb6L_Z3OEyZy",
    "outputId": "02b2b2b8-0d87-4da2-d6d9-a047a080dd16"
   },
   "outputs": [],
   "source": [
    "diagnosis = {x:x[6:] for x in diagnosis}\n",
    "diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1734502096925,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "NzCH3eGLEycc",
    "outputId": "69a5fd91-6acd-45f5-bd47-28dbe538cbb6"
   },
   "outputs": [],
   "source": [
    "RF = RF.rename(columns=diagnosis)\n",
    "RF.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1734502100146,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "WqOMuewsEyfF",
    "outputId": "2ee4de8e-75f1-453c-c220-be6ba19e9a47"
   },
   "outputs": [],
   "source": [
    "F = pd.concat([LF,RF],axis=0,ignore_index=True)\n",
    "F.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "executionInfo": {
     "elapsed": 264,
     "status": "ok",
     "timestamp": 1734502103238,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "rQa4FWKmE-sE",
    "outputId": "58e59e38-f24a-4d85-c4ee-4af712ce0aa1"
   },
   "outputs": [],
   "source": [
    "F = F.merge(df_meta_f[['series_id','series_description']], left_on='series_id', right_on='series_id')\n",
    "F.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 249,
     "status": "ok",
     "timestamp": 1734502106211,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "3fXdu18IFLNn",
    "outputId": "6124fccc-960b-4bd3-919f-f203aa55f251"
   },
   "outputs": [],
   "source": [
    "target = F.columns[-6:-1]\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StVFvsi4Y-mu"
   },
   "outputs": [],
   "source": [
    "labels = {\n",
    "    'Normal/Mild':0,\n",
    "    'Moderate':1,\n",
    "    'Severe':2\n",
    "}\n",
    "\n",
    "coor = [\n",
    "    'x_L1L2',\n",
    "    'y_L1L2',\n",
    "    'x_L2L3',\n",
    "    'y_L2L3',\n",
    "    'x_L3L4',\n",
    "    'y_L3L4',\n",
    "    'x_L4L5',\n",
    "    'y_L4L5',\n",
    "    'x_L5S1',\n",
    "    'y_L5S1',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STllF8rGFVa8"
   },
   "outputs": [],
   "source": [
    "class T1Dataset(Dataset):\n",
    "    def __init__(self, df, VALID=False, alpha=0):\n",
    "        self.data = df\n",
    "        self.VALID = VALID\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]#1行目を取得\n",
    "\n",
    "        centers = torch.as_tensor([x for x in row[coor]]).view(5,2).float()#病気だと判定した中心座標を取得\n",
    "\n",
    "        sample = '/content/train_images/'\n",
    "        sample = sample+str(row['study_id'])+'/'+str(row['series_id'])+'/'+str(row['instance_number'])+'.dcm'\n",
    "        image = pydicom.dcmread(sample).pixel_array#pydicomで画像を読み込む\n",
    "        H,W = image.shape\n",
    "\n",
    "#画像を正方形になるように切り取る。切り取った分中心座標の位置を調整する。\n",
    "        if H > W:\n",
    "            d = W\n",
    "            if not self.VALID:\n",
    "                h = int((H - d)*(.5 + self.alpha*(.5 - np.random.rand())))\n",
    "            else:\n",
    "                h = (H - d)//2\n",
    "            image = image[h:h+d]\n",
    "            centers[:,1] -= h\n",
    "            H = W\n",
    "        elif H < W:\n",
    "            d = H\n",
    "            if not self.VALID:\n",
    "                w = int((W - d)*(.5 + self.alpha*(.5 - np.random.rand())))\n",
    "            else:\n",
    "                w = (W - d)//2\n",
    "            image = image[:,w:w+d]\n",
    "            centers[:,0] -= w\n",
    "            W = H\n",
    "\n",
    "        image = cv2.resize(image,(PATCH_SIZE,PATCH_SIZE))#画像をPATCH_SIZE*PATCH_SIZEにする\n",
    "        image = torch.as_tensor(image/np.max(image)).unsqueeze(0).float()\n",
    "\n",
    "        label = torch.as_tensor([labels[x] for x in row[target]])#labelは使用しないが適当になにかしら入れておく。\n",
    "\n",
    "        #中心座標の位置を調整する。\n",
    "\n",
    "        centers[:,0] = centers[:,0]*PATCH_SIZE/W\n",
    "        centers[:,1] = centers[:,1]*PATCH_SIZE/H\n",
    "\n",
    "        return image.to(device),[label.to(device),centers.to(device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mhehRysYmkQ"
   },
   "outputs": [],
   "source": [
    "SEG_TRAIN = True#セグメンテーションの学習をしたいときはここをTrueにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "executionInfo": {
     "elapsed": 234878,
     "status": "error",
     "timestamp": 1734502678734,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "xmhRa2TkFVdR",
    "outputId": "8e1770c8-7b83-4169-f6df-d35c92c1ab96"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "tdf = F[F['series_description'] != fold]#trainのデータを持ってくる。\n",
    "vdf = F[F['series_description'] == fold]#validのデータを持ってくる。\n",
    "\n",
    "# tdf2=df_melted[df_melted['series_description'] != fold]\n",
    "# vdf2=df_melted[df_melted['series_description'] == fold]\n",
    "\n",
    "tds = T1Dataset(tdf)#定義したdataset関数からtrainのデータを持ってくる。\n",
    "vds = T1Dataset(vdf,VALID=True)#定義したdataset関数からvalidのデータを持ってくる。\n",
    "tdl = torch.utils.data.DataLoader(tds, batch_size=SEG['BS'], shuffle=True, drop_last=True)#バッチサイズを指定してデータローダー化する\n",
    "vdl = torch.utils.data.DataLoader(vds, batch_size=SEG['BS'], shuffle=False)#バッチサイズを指定してデータローダー化する\n",
    "\n",
    "if SEG_TRAIN:\n",
    "    seed_everything(SEED)\n",
    "\n",
    "    dls = DataLoaders(tdl,vdl)#fastai用にデータローダーを定義する。\n",
    "\n",
    "    n_iter = len(tds)//SEG['BS']\n",
    "\n",
    "    model = myUNet()\n",
    "    learn = Learner(\n",
    "        dls,\n",
    "        model,\n",
    "        lr=SEG['LR'],#学習率を設定\n",
    "        loss_func=myLoss(alpha=0.5),\n",
    "        # cbs=[\n",
    "        #     ShowGraphCallback(),#学習曲線を表示\n",
    "        #     alpha_cb\n",
    "        # ]\n",
    "    )\n",
    "    learn.fit_one_cycle(SEG['EPOCHS'])#エポック数を設定\n",
    "    with open('/content/drive/MyDrive/RSNA_csv/'+\"SEG_\"+str(fold)+\"_sagt1_rev\"+\".pkl\", 'wb') as f:#モデルを回したら保存する。\n",
    "      pickle.dump(model, f)\n",
    "    del tdl,vdl,dls,model,learn #終わったらモデルを全て消す（メモリを使いすぎないため）\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DZ0Mbhw8jXP"
   },
   "source": [
    "# saggitalT1画像の前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Ktjz3IT_VjZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# データセットのパス\n",
    "base_dir = '/content/train_images'\n",
    "\n",
    "# リストを初期化\n",
    "data = []\n",
    "\n",
    "# 一番上の階層（study_id）のフォルダをたどる\n",
    "for study_id in os.listdir(base_dir):\n",
    "    study_path = os.path.join(base_dir, study_id)\n",
    "    if os.path.isdir(study_path):\n",
    "        # 二番目の階層（series_id）のフォルダをたどる\n",
    "        for series_id in os.listdir(study_path):\n",
    "            series_path = os.path.join(study_path, series_id)\n",
    "            if os.path.isdir(series_path):\n",
    "                # 三番目の階層（〇〇.dcm ファイル）をたどる\n",
    "                for filename in os.listdir(series_path):\n",
    "                    if filename.endswith('.dcm'):\n",
    "                        # 〇〇.dcm の〇〇部分（instance_number）を抽出\n",
    "                        instance_number = filename.split('.')[0]\n",
    "                        # データを追加\n",
    "                        data.append([study_id, series_id, instance_number])\n",
    "\n",
    "# pandas DataFrameを作成\n",
    "df = pd.DataFrame(data, columns=['study_id', 'series_id', 'instance_number'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_AjZpfvlAsbi"
   },
   "outputs": [],
   "source": [
    "df_meta_f = pd.read_csv('/content/train_series_descriptions.csv')\n",
    "df_meta_f.tail()\n",
    "\n",
    "df = df.astype('int64')\n",
    "\n",
    "all_df = pd.merge(df, df_meta_f, on=['study_id', 'series_id'], how='inner')\n",
    "\n",
    "s_all_df=all_df[all_df['series_description']=='Sagittal T1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMaKGg28cF1o"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# データフレームをコピーして操作\n",
    "s_all_df_copy = s_all_df.copy()\n",
    "\n",
    "# x_pos カラムを NaN で初期化\n",
    "s_all_df_copy['x_pos'] = None\n",
    "s_all_df_copy['y_pos'] = None\n",
    "s_all_df_copy['z_pos'] = None\n",
    "s_all_df_copy['pixel_sp_z'] = None\n",
    "s_all_df_copy['pixel_sp_y'] = None\n",
    "# 各行を処理\n",
    "for idx, row in s_all_df_copy.iterrows():\n",
    "    # DICOMファイルのパスを構築\n",
    "    dicom_file_path = f\"/content/train_images/{row['study_id']}/{row['series_id']}/{row['instance_number']}.dcm\"\n",
    "\n",
    "    # DICOMファイルを読み込む\n",
    "    dicom_data = pydicom.dcmread(dicom_file_path)\n",
    "\n",
    "    # 'Image Position (Patient)' の x 座標を取得して x_pos カラムに格納\n",
    "    s_all_df_copy.loc[idx, 'x_pos'] = dicom_data.ImagePositionPatient[0]\n",
    "    s_all_df_copy.loc[idx, 'y_pos'] = dicom_data.ImagePositionPatient[1]\n",
    "    s_all_df_copy.loc[idx, 'z_pos'] = dicom_data.ImagePositionPatient[2]\n",
    "    s_all_df_copy.loc[idx, 'pixel_sp_z']=dicom_data.PixelSpacing[0]\n",
    "    s_all_df_copy.loc[idx, 'pixel_sp_y']=dicom_data.PixelSpacing[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3546,
     "status": "ok",
     "timestamp": 1734573476506,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "bia3g1yv_0qp",
    "outputId": "bee1eb0e-919e-4faa-9fd6-1cbfe6b991de"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 骨の画像を正しい順番（手から体の中心部に行くような順番）に並び替える\n",
    "def assign_description(group):\n",
    "    # x_posを基準に小さい順に並び替え\n",
    "    group = group.sort_values(by='x_pos').reset_index(drop=True)\n",
    "\n",
    "    # 前半と後半を分割するインデックス\n",
    "    midpoint = len(group) // 2\n",
    "\n",
    "    # 前半を'Right Neural Foraminal Narrowing'\n",
    "    group.loc[:midpoint-1, 'description'] = 'Right Neural Foraminal Narrowing'\n",
    "\n",
    "    # 後半を'Left Neural Foraminal Narrowing'\n",
    "    group.loc[midpoint:, 'description'] = 'Left Neural Foraminal Narrowing'\n",
    "\n",
    "    return group\n",
    "\n",
    "# study_id, series_idごとにグループ化し、assign_description関数を適用\n",
    "s_all_df = s_all_df_copy.groupby(['study_id']).apply(assign_description).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3626,
     "status": "ok",
     "timestamp": 1734573486027,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "f5yCQKxLARgv",
    "outputId": "79077c68-409d-4f43-91fd-d04d3e842b46"
   },
   "outputs": [],
   "source": [
    "def sort_based_on_description(df):\n",
    "    # \"Right Neural Foraminal Narrowing\" は x_pos が小さい順\n",
    "    df_right = df[df['description'] == 'Right Neural Foraminal Narrowing'].sort_values(by='x_pos', ascending=True)\n",
    "\n",
    "    # \"Left Neural Foraminal Narrowing\" は x_pos が大きい順\n",
    "    df_left = df[df['description'] == 'Left Neural Foraminal Narrowing'].sort_values(by='x_pos', ascending=False)\n",
    "\n",
    "    # 結合して返す\n",
    "    return pd.concat([df_right, df_left])\n",
    "\n",
    "# study_id, series_idごとにグループ化し、上記の関数で並び替え\n",
    "sorted_s_all_df2 = s_all_df.groupby(['study_id', 'series_id'], group_keys=False).apply(sort_based_on_description)\n",
    "\n",
    "# 結果を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2273,
     "status": "ok",
     "timestamp": 1734573493000,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "U5L1_jNQAv1e",
    "outputId": "423f95cb-b670-4af5-b413-99f56ed19f50"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# グループごとに均等な間隔で10個の行を選択する関数\n",
    "def select_evenly_spaced(group, num=8):\n",
    "    # グループのサイズが10個未満の場合はそのまま返す\n",
    "    if len(group) <= num:\n",
    "        return group\n",
    "    # ステップを計算し、均等な間隔でデータを選ぶ\n",
    "    step = len(group) / num\n",
    "    indices = [int(i * step) for i in range(num)]\n",
    "    return group.iloc[indices]\n",
    "\n",
    "# study_id, instance_number, descriptionでグループ化し、各グループから均等に10個選択\n",
    "new_df =sorted_s_all_df2.groupby(['study_id', 'description'], group_keys=False).apply(select_evenly_spaced)\n",
    "\n",
    "# 結果の確認\n",
    "new_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1004,
     "status": "ok",
     "timestamp": 1734573530652,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "mS-IdXZfCjRx",
    "outputId": "b6a9647f-fe51-46b5-a1cf-f6d89a63fa82"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 各study_idグループに8個以上の行が入るようにするための処理\n",
    "def ensure_min_rows(group, min_rows=8):\n",
    "    if len(group) < min_rows:\n",
    "        #8個より少ない場合はランダムにそのグループ内から8個になるように付け足す。\n",
    "        additional_rows = group.sample(n=min_rows - len(group), replace=True, random_state=42)\n",
    "        group = pd.concat([group, additional_rows], ignore_index=True)\n",
    "    return group\n",
    "\n",
    "new_df_padded = new_df.groupby(['study_id', 'description'], group_keys=False).apply(ensure_min_rows)\n",
    "\n",
    "print(new_df_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1734573547993,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "ciXenl0KDAtC",
    "outputId": "629df298-1725-4f3e-b419-08cf0e3f570d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# s_all_df3の5つのコピーを作成し、それぞれにlevelカラムを追加\n",
    "dfs = [new_df_padded.copy().assign(level=i) for i in range(5)]\n",
    "\n",
    "# それらのデータフレームを縦に連結\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 結果の確認\n",
    "print(combined_df.head())\n",
    "print(combined_df['level'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "limDP359aBA1"
   },
   "outputs": [],
   "source": [
    "merged_df = pd.merge(combined_df,train, on='study_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4982,
     "status": "ok",
     "timestamp": 1734573847023,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "wK2PFBoJDHU3",
    "outputId": "3a1b9aa0-bf98-4b3a-9b84-e9dd4b68284e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# サンプルデータフレームの作成（ここでは仮のカラムを使用しています）\n",
    "# 実際には combined_df には適切なカラムが含まれている必要があります\n",
    "# combined_df = pd.DataFrame({\n",
    "#     'description': [...],\n",
    "#     'level': [...],\n",
    "#     'left_neural_foraminal_narrowing_l1_l2': [...],\n",
    "#     'left_neural_foraminal_narrowing_l2_l3': [...],\n",
    "#     'left_neural_foraminal_narrowing_l3_l4': [...],\n",
    "#     'left_neural_foraminal_narrowing_l4_l5': [...],\n",
    "#     'left_neural_foraminal_narrowing_l5_s1': [...],\n",
    "#     'right_neural_foraminal_narrowing_l1_l2': [...],\n",
    "#     'right_neural_foraminal_narrowing_l2_l3': [...],\n",
    "#     'right_neural_foraminal_narrowing_l3_l4': [...],\n",
    "#     'right_neural_foraminal_narrowing_l4_l5': [...],\n",
    "#     'right_neural_foraminal_narrowing_l5_s1': [...],\n",
    "# })\n",
    "\n",
    "# labelにはtrain.csvで明示されている正解データを格納する\n",
    "def get_label(row):\n",
    "    if row['description'] == 'Left Neural Foraminal Narrowing':\n",
    "        if row['level'] == 0:\n",
    "            return row['left_neural_foraminal_narrowing_l1_l2']\n",
    "        elif row['level'] == 1:\n",
    "            return row['left_neural_foraminal_narrowing_l2_l3']\n",
    "        elif row['level'] == 2:\n",
    "            return row['left_neural_foraminal_narrowing_l3_l4']\n",
    "        elif row['level'] == 3:\n",
    "            return row['left_neural_foraminal_narrowing_l4_l5']\n",
    "        elif row['level'] == 4:\n",
    "            return row['left_neural_foraminal_narrowing_l5_s1']\n",
    "    elif row['description'] == 'Right Neural Foraminal Narrowing':\n",
    "        if row['level'] == 0:\n",
    "            return row['right_neural_foraminal_narrowing_l1_l2']\n",
    "        elif row['level'] == 1:\n",
    "            return row['right_neural_foraminal_narrowing_l2_l3']\n",
    "        elif row['level'] == 2:\n",
    "            return row['right_neural_foraminal_narrowing_l3_l4']\n",
    "        elif row['level'] == 3:\n",
    "            return row['right_neural_foraminal_narrowing_l4_l5']\n",
    "        elif row['level'] == 4:\n",
    "            return row['right_neural_foraminal_narrowing_l5_s1']\n",
    "    return np.nan  # 予期しない値の場合はNaNを返す\n",
    "\n",
    "# 新しいカラム `label` を作成\n",
    "merged_df['label'] = merged_df.apply(get_label, axis=1)\n",
    "\n",
    "# 結果の確認\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucMwz79pqIPS"
   },
   "outputs": [],
   "source": [
    "combined_df=merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20040,
     "status": "ok",
     "timestamp": 1734573891056,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "S9U9XZBTKO0j",
    "outputId": "8863ba11-720f-4094-da31-bdd513791835"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 例として final_df を作成（実際には適切なデータフレームを使用します）\n",
    "# final_df = pd.DataFrame({\n",
    "#     'description': [...],\n",
    "#     'level': [...],\n",
    "#     'label': [...],\n",
    "#     'instance_number': [...],\n",
    "#     'other_col1': [...],\n",
    "#     'other_col2': [...],\n",
    "#     ...\n",
    "# })\n",
    "\n",
    "# 1行にまとめるための処理\n",
    "\n",
    "def group_rows(df, group_size):\n",
    "    grouped = []\n",
    "    num_groups = len(df) // group_size + int(len(df) % group_size != 0)\n",
    "\n",
    "    for i in range(num_groups):\n",
    "        start_idx = i * group_size\n",
    "        end_idx = min((i + 1) * group_size, len(df))\n",
    "        subset = df.iloc[start_idx:end_idx].reset_index(drop=True)\n",
    "\n",
    "        # グループ内の先頭行の値を取得\n",
    "        first_row = subset.iloc[0]\n",
    "\n",
    "        # 新しいカラムに instance_number を格納\n",
    "        row_data = first_row.to_dict()  # 他の列の情報も含めて最初の行の情報を取得\n",
    "        row_data.update({\n",
    "            f'instance_number_{j}': subset.iloc[j]['instance_number'] for j in range(len(subset))\n",
    "        })\n",
    "\n",
    "        # フィルの中身をリストに格納\n",
    "        grouped.append(row_data)\n",
    "\n",
    "    return pd.DataFrame(grouped)\n",
    "\n",
    "# グループサイズを設定（5つの行を1行にまとめる）\n",
    "group_size = 8\n",
    "grouped_df = group_rows(combined_df, group_size)\n",
    "\n",
    "print(grouped_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3K3Yp38qPWM"
   },
   "outputs": [],
   "source": [
    "grouped_df=grouped_df[grouped_df['label'].notna()]#ラベルがついていないものは排除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvCRaJL0ZB4c"
   },
   "outputs": [],
   "source": [
    "grouped_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1734586160054,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "KEKKZM5eK8uW",
    "outputId": "55017785-5757-4b4b-a4aa-3158ce207310"
   },
   "outputs": [],
   "source": [
    "# DataFrameを保存するパス\n",
    "save_path = '/content/drive/MyDrive/RSNA_csv/grouped_df_rev.pkl'\n",
    "\n",
    "# DataFrameをpkl形式で保存\n",
    "grouped_df.to_pickle(save_path)\n",
    "\n",
    "print(f\"DataFrame has been saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNNhbevYMn6q"
   },
   "source": [
    "# saggitalT1画像の真ん中だけを取ってきたものの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1836,
     "status": "ok",
     "timestamp": 1734574041505,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "TJN76HU-B3CT",
    "outputId": "347c2d98-8dae-4fbb-91ff-e95c6a61a2ea"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# s_all_dfをstudy_idごとにグループ化し、instance_numberでソートし、真ん中の要素を取得\n",
    "def get_middle_instance(df):\n",
    "    middle_rows = []\n",
    "\n",
    "    # study_idごとにグループ化\n",
    "    grouped = df.groupby('study_id')\n",
    "\n",
    "    for _, group in grouped:\n",
    "        # instance_numberで昇順にソート\n",
    "        sorted_group = group.sort_values(by='instance_number').reset_index(drop=True)\n",
    "\n",
    "        # 真ん中のインデックスを計算\n",
    "        midpoint = len(sorted_group) // 2\n",
    "\n",
    "        # 真ん中の行を取得\n",
    "        middle_row = sorted_group.iloc[midpoint]\n",
    "\n",
    "        # 真ん中の行をリストに追加\n",
    "        middle_rows.append(middle_row)\n",
    "\n",
    "    # すべての真ん中の行を結合して新しいDataFrameを作成\n",
    "    middle_df = pd.DataFrame(middle_rows)\n",
    "\n",
    "    return middle_df\n",
    "\n",
    "middle_s_all_df = get_middle_instance(s_all_df_copy)\n",
    "\n",
    "# 結果を表示\n",
    "print(middle_s_all_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTHzPaLgMlHu"
   },
   "outputs": [],
   "source": [
    "middle_s_all_df =  middle_s_all_df.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iy4k3ifFrLpe"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/RSNA_csv/'+\"SEG_\"+\"1\"+\".pkl\", 'rb') as f:\n",
    "      model1=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 121351,
     "status": "ok",
     "timestamp": 1734574378185,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "ZHx5khgeM4Yt",
    "outputId": "d0540d75-e038-484e-8c85-01192fc1f582"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# (OUT > 0.5)[0][1] がテンソルマスクだと仮定\n",
    "for j in range(len(middle_s_all_df)):\n",
    "  image=pydicom.dcmread(\"/content/train_images/\"+str(middle_s_all_df.iloc[j][\"study_id\"])+\"/\"+str(middle_s_all_df.iloc[j][\"series_id\"])+\"/\"+str(middle_s_all_df.iloc[j][\"instance_number\"])+\".dcm\").pixel_array\n",
    "  dicom=pydicom.dcmread(\"/content/train_images/\"+str(middle_s_all_df.iloc[j][\"study_id\"])+\"/\"+str(middle_s_all_df.iloc[j][\"series_id\"])+\"/\"+str(middle_s_all_df.iloc[j][\"instance_number\"])+\".dcm\")\n",
    "  H,W = image.shape\n",
    "  middle_s_all_df.loc[j, 'H'] = H\n",
    "  middle_s_all_df.loc[j, 'W'] = W\n",
    "  middle_s_all_df.loc[j,'o0']=dicom.ImageOrientationPatient[0]\n",
    "  middle_s_all_df.loc[j,'o1']=dicom.ImageOrientationPatient[1]\n",
    "  middle_s_all_df.loc[j,'o2']=dicom.ImageOrientationPatient[2]\n",
    "  middle_s_all_df.loc[j,'o3']=dicom.ImageOrientationPatient[3]\n",
    "  middle_s_all_df.loc[j,'o4']=dicom.ImageOrientationPatient[4]\n",
    "  middle_s_all_df.loc[j,'o5']=dicom.ImageOrientationPatient[5]\n",
    "  middle_s_all_df.loc[j, 'H_cut']=0\n",
    "  middle_s_all_df.loc[j, 'W_cut']=0\n",
    "  # centers = torch.as_tensor([x for x in row[coor]]).view(5,2).float()\n",
    "  # By plane resizing I've been distorting the proportions\n",
    "  if H > W:\n",
    "      d = W\n",
    "      h = (H - d)//2\n",
    "      image = image[h:h+d]\n",
    "      middle_s_all_df.loc[j, 'H_cut'] = h\n",
    "      # centers[:,1] -= h\n",
    "      H = W\n",
    "  elif H < W:\n",
    "      d = H\n",
    "      w = (W - d)//2\n",
    "      image = image[:,w:w+d]\n",
    "      middle_s_all_df.loc[j, 'W_cut'] = w\n",
    "      # centers[:,0] -= w\n",
    "      W = H\n",
    "  image = cv2.resize(image,(PATCH_SIZE,PATCH_SIZE))\n",
    "  image = torch.as_tensor(image/np.max(image)).unsqueeze(0).float()\n",
    "  image=image.to(device)\n",
    "  OUT = model1(image.unsqueeze(0)).cpu().detach()\n",
    "  for i in range(5):\n",
    "    mask = (OUT > 0.5)[0][i]  # 512x512のテンソル\n",
    "\n",
    "    # Trueのピクセル座標を取得（y座標とx座標のペアとして取得）\n",
    "    true_pixels = torch.nonzero(mask)\n",
    "\n",
    "    # 平均座標を計算\n",
    "    mean_yx = true_pixels.float().mean(dim=0)\n",
    "\n",
    "    # 平均座標を表示\n",
    "    mean_y, mean_x = mean_yx[0], mean_yx[1]\n",
    "    print(f\"Mean coordinates: (y, x) = ({0-mean_y.item()}, {mean_x.item()})\")\n",
    "\n",
    "    middle_s_all_df.loc[j, f'mean_y_{i}']=mean_y.item()\n",
    "    middle_s_all_df.loc[j, f'mean_x_{i}']=mean_x.item()\n",
    "\n",
    "    middle_s_all_df.loc[j, f'z_pos_{i}'] = middle_s_all_df.loc[j, 'pixel_sp_z'] * (0 - mean_y.item()) + middle_s_all_df.loc[j, 'z_pos']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXW4VSYaNl1w"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 各マスクの座標を元の画像サイズに戻すコード\n",
    "for j in range(len(middle_s_all_df)):\n",
    "    # 元の画像サイズ\n",
    "    H = middle_s_all_df.loc[j, 'H']\n",
    "    W = middle_s_all_df.loc[j, 'W']\n",
    "\n",
    "    # 512にリサイズされた画像のサイズ\n",
    "    resized_size = 512\n",
    "\n",
    "    # 高さと幅のスケール（リサイズ前の座標に戻すためのスケール）\n",
    "    if H<W:\n",
    "      scale_y = H / resized_size\n",
    "      scale_x = H / resized_size\n",
    "    elif H>W:\n",
    "      scale_y = W / resized_size\n",
    "      scale_x = W / resized_size\n",
    "    else:\n",
    "      scale_y = W / resized_size\n",
    "      scale_x = H / resized_size\n",
    "    # 5つのマスクについて座標を戻す\n",
    "    for i in range(5):\n",
    "        # リサイズ後の座標を取得\n",
    "        mean_y_resized = middle_s_all_df.loc[j, f'mean_y_{i}']\n",
    "        mean_x_resized = middle_s_all_df.loc[j, f'mean_x_{i}']\n",
    "\n",
    "        # リサイズ前の座標に戻す（スケールを掛ける）\n",
    "        mean_bfr_y = mean_y_resized * scale_y\n",
    "        mean_bfr_x = mean_x_resized * scale_x\n",
    "\n",
    "        # 戻した座標を新しいカラムに格納\n",
    "        middle_s_all_df.loc[j, f'mean_bfr_y_{i}'] = mean_bfr_y\n",
    "        middle_s_all_df.loc[j, f'mean_bfr_x_{i}'] = mean_bfr_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CEPxQCl-N5Mq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 各マスクの切り取る前の座標を計算して新しいカラムに格納\n",
    "for j in range(len(middle_s_all_df)):\n",
    "    # H_cut, W_cut はどれくらい切り取られたかを表す（NaNの場合は切り取りなし）\n",
    "    H_cut = middle_s_all_df.loc[j, 'H_cut'] if not pd.isna(middle_s_all_df.loc[j, 'H_cut']) else 0\n",
    "    W_cut = middle_s_all_df.loc[j, 'W_cut'] if not pd.isna(middle_s_all_df.loc[j, 'W_cut']) else 0\n",
    "\n",
    "    for i in range(5):\n",
    "        # リサイズ前の平均座標を取得\n",
    "        mean_bfr_y = middle_s_all_df.loc[j, f'mean_bfr_y_{i}']\n",
    "        mean_bfr_x = middle_s_all_df.loc[j, f'mean_bfr_x_{i}']\n",
    "\n",
    "        # 切り取る前の座標を計算（H_cutとW_cutを戻す）\n",
    "        mean_bfr_cut_y = mean_bfr_y + H_cut\n",
    "        mean_bfr_cut_x = mean_bfr_x + W_cut\n",
    "\n",
    "        # 切り取る前の座標を新しいカラムに格納\n",
    "        middle_s_all_df.loc[j, f'mean_bfr_cut_y_{i}'] = mean_bfr_cut_y\n",
    "        middle_s_all_df.loc[j, f'mean_bfr_cut_x_{i}'] = mean_bfr_cut_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouHVCxJsN6WG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 3次元座標を計算してカラムに格納するコード\n",
    "for j in range(len(middle_s_all_df)):\n",
    "    # DICOMのメタデータ\n",
    "    x_pos = middle_s_all_df.loc[j, 'x_pos']\n",
    "    y_pos = middle_s_all_df.loc[j, 'y_pos']\n",
    "    z_pos = middle_s_all_df.loc[j, 'z_pos']\n",
    "\n",
    "    o0, o1, o2 = middle_s_all_df.loc[j, ['o0', 'o1', 'o2']]  # 行方向ベクトル (ImageOrientationPatientの0〜2番目)\n",
    "    o3, o4, o5 = middle_s_all_df.loc[j, ['o3', 'o4', 'o5']]  # 列方向ベクトル (ImageOrientationPatientの3〜5番目)\n",
    "\n",
    "    pixel_sp_y = middle_s_all_df.loc[j, 'pixel_sp_y']  # 行方向のPixelSpacing\n",
    "    pixel_sp_z = middle_s_all_df.loc[j, 'pixel_sp_z']  # 列方向のPixelSpacing\n",
    "\n",
    "    # 行方向ベクトル\n",
    "    row_direction = np.array([o0, o1, o2])\n",
    "    # 列方向ベクトル\n",
    "    col_direction = np.array([o3, o4, o5])\n",
    "\n",
    "    # 各マスクに対して3次元座標を計算\n",
    "    for i in range(5):\n",
    "        # ピクセル座標を取得\n",
    "        mean_y = middle_s_all_df.loc[j, f'mean_bfr_cut_y_{i}']  # 行方向（R）\n",
    "        mean_x = middle_s_all_df.loc[j, f'mean_bfr_cut_x_{i}']  # 列方向（C）\n",
    "\n",
    "        # 3次元座標を計算\n",
    "        patient_pos = np.array([x_pos, y_pos, z_pos]) + \\\n",
    "                      (pixel_sp_y * mean_x) * row_direction + \\\n",
    "                      (pixel_sp_z * mean_y) * col_direction\n",
    "\n",
    "        # 計算した3次元座標をカラムに格納\n",
    "        middle_s_all_df.loc[j, f'xx_{i}'] = patient_pos[0]\n",
    "        middle_s_all_df.loc[j, f'yy_{i}'] = patient_pos[1]\n",
    "        middle_s_all_df.loc[j, f'zz_{i}'] = patient_pos[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1734574489842,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "_OkBOuvcOJKH",
    "outputId": "30679e63-7ad3-4b0e-df55-9c0e64ceeeb0"
   },
   "outputs": [],
   "source": [
    "# DataFrameを保存するパス\n",
    "save_path = '/content/drive/MyDrive/RSNA_csv/middle_s_all_df_rev.pkl'\n",
    "\n",
    "# DataFrameをpkl形式で保存\n",
    "middle_s_all_df.to_pickle(save_path)\n",
    "\n",
    "print(f\"DataFrame has been saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VciBEiIqLYwh"
   },
   "source": [
    "# 病気を判定するモデル学習部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0tdbo15SLiaf"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"/content/drive/MyDrive/RSNA_csv/grouped_df_rev.pkl\", 'rb') as f:\n",
    "  grouped_df=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YCZo6clNWFeR"
   },
   "outputs": [],
   "source": [
    "unique_studies = grouped_df['study_id'].unique()\n",
    "study_mapping = {study: (i % 5) + 1 for i, study in enumerate(unique_studies)}\n",
    "\n",
    "# study_idごとにfoldをわける\n",
    "grouped_df['series_description2'] = grouped_df['study_id'].map(study_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zwh_vc2U5Sa"
   },
   "outputs": [],
   "source": [
    "tdf2=grouped_df[grouped_df['series_description2'] != fold]\n",
    "vdf2=grouped_df[grouped_df['series_description2'] == fold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSU_xQCnb0FE"
   },
   "outputs": [],
   "source": [
    "vdf2.to_pickle(\"/content/drive/MyDrive/RSNA_csv/grouped_df_sagt1_vdf2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1137,
     "status": "ok",
     "timestamp": 1736911695236,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "7TqwkF-TSWma",
    "outputId": "cff9908a-86d6-4dbb-e200-39b05472fbb7"
   },
   "outputs": [],
   "source": [
    "import albumentations as A #データ拡張の処理をする\n",
    "\n",
    "AUG_PROB = 0.75 #75%の確率でデータ拡張する\n",
    "transforms_train = A.Compose([\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=AUG_PROB),#明るさやコントラストを20%の確率で変更\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(blur_limit=5),#動きによるぼかしをいれる\n",
    "        # A.MedianBlur(blur_limit=5),\n",
    "        A.GaussianBlur(blur_limit=5),#ガウスフィルタによるぼかしを入れる\n",
    "        A.GaussNoise(var_limit=(5.0, 30.0)),#ランダムなガウスノイズを入れる\n",
    "    ], p=AUG_PROB),\n",
    "\n",
    "    A.OneOf([\n",
    "        A.OpticalDistortion(distort_limit=1.0),#光学的な歪みを実現\n",
    "        A.GridDistortion(num_steps=5, distort_limit=1.),#グリッドごとにゆがませる\n",
    "        A.ElasticTransform(alpha=3),#弾性変形を適用\n",
    "    ], p=AUG_PROB),\n",
    "\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=AUG_PROB),#平行移動、拡大縮小、回転\n",
    "  #  A.CoarseDropout(max_holes=15, max_height=30, max_width=30, min_holes=1, min_height=8, min_width=8, p=AUG_PROB),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KwfsQfsGdgu0"
   },
   "outputs": [],
   "source": [
    "patch_size = 90 #切り取るサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IV4n_40gL4qb"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class ViT_T1_Dataset(Dataset):\n",
    "    def __init__(self, df, UNet,transform=None, VALID=False, P=patch_size, alpha=0):\n",
    "        self.data = df\n",
    "        self.UNet = UNet\n",
    "        self.VALID = VALID\n",
    "        self.P = P\n",
    "        self.alpha = alpha\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "      x = np.zeros((8, self.P, self.P), dtype=np.float32)#まずは全部0が入っているものを作るサイズはpatch_size*patch_size\n",
    "      non_zero_slice=[]\n",
    "\n",
    "      for i in range(8):\n",
    "\n",
    "        sample = '/content/train_images/'\n",
    "        sample = sample+str(self.data.iloc[index]['study_id'])+'/'+str(self.data.iloc[index]['series_id'])+'/'+str(self.data.iloc[index][f'instance_number_{i}'])+'.dcm'\n",
    "\n",
    "        image = pydicom.dcmread(sample).pixel_array\n",
    "        H,W = image.shape\n",
    "        # centers = torch.as_tensor([x for x in row[coor]]).view(5,2).float()\n",
    "        # 正方形に切り取る\n",
    "        if H > W:\n",
    "            d = W\n",
    "            h = (H - d)//2\n",
    "            image = image[h:h+d]\n",
    "            # centers[:,1] -= h\n",
    "            H = W\n",
    "        elif H < W:\n",
    "            d = H\n",
    "            w = (W - d)//2\n",
    "            image = image[:,w:w+d]\n",
    "            # centers[:,0] -= w\n",
    "            W = H\n",
    "        image = cv2.resize(image,(PATCH_SIZE,PATCH_SIZE))\n",
    "        image = torch.as_tensor(image/(np.max(image))).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "\n",
    "        # OUT = 0\n",
    "        # with torch.no_grad():\n",
    "                # for rot in [0,1,2,3]:\n",
    "                        # OUT += torch.rot90(self.UNet(torch.rot90(image,rot,dims=[-2, -1])),-rot,dims=[-2, -1])\n",
    "        OUT=self.UNet(image)\n",
    "        OUT = (OUT > TH)[0]#閾値0.5でマスクを0と1に分ける\n",
    "        c = (OUT.unsqueeze(1)*idx_map[0]).view(5,2,PATCH_SIZE*PATCH_SIZE).sum(-1)\n",
    "        d = OUT.view(5,PATCH_SIZE*PATCH_SIZE).sum(-1)\n",
    "        m = d > 0\n",
    "        c[m] = (c[m]/(d[m]).unsqueeze(-1)).long()\n",
    "        c[~m] = self.P\n",
    "\n",
    "        image_slices = []\n",
    "\n",
    "        for xy in c:\n",
    "          y_start = max(0, xy[1] - self.P // 2)\n",
    "          y_end = min(512,xy[1] + self.P - self.P // 2)\n",
    "          x_start = max(0, xy[0] - self.P // 2)\n",
    "          x_end = min(512,xy[0] + self.P - self.P // 2)\n",
    "          if (y_end - y_start == self.P) and (x_end - x_start == self.P):#切り取りの部分が途中で切れていないない場合\n",
    "            slice_img = image[0, 0, y_start:y_end, x_start:x_end]\n",
    "            non_zero_slice.append(slice_img)\n",
    "            image_slices.append(slice_img)\n",
    "          else:#切り取りの部分が途中で切れている場合\n",
    "            zero_slice = torch.zeros((self.P, self.P), device=image.device)\n",
    "            image_slices.append(zero_slice)\n",
    "            # print(f\"Skipped slice due to incorrect size: {(y_end - y_start, x_end - x_start)}\")\n",
    "            # print(f\"Slice coordinates: {(xy[1], xy[0])}\")\n",
    "\n",
    "# スライスがあればスタックする\n",
    "        if image_slices:\n",
    "          try:\n",
    "            image = torch.stack(image_slices)\n",
    "          except RuntimeError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        else:\n",
    "          print(\"No valid slices available for stacking.\")\n",
    "\n",
    "        # if not self.VALID: image = augment_image(image,self.alpha)\n",
    "        x[i,...]=image[self.data.iloc[index]['level']].cpu().numpy()\n",
    "      for i in range(8):\n",
    "        if (x[i,...].sum() == 0) and (len(non_zero_slice)>0):\n",
    "          x[i,...] = non_zero_slice[0].cpu().numpy()\n",
    "        else:\n",
    "          pass\n",
    "        #nothing\n",
    "      if self.transform is not None:\n",
    "        x = self.transform(image=x)['image']#albumentationを適用\n",
    "      x=torch.as_tensor(x).float()\n",
    "      x = F.interpolate(x.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)#モデルに適用するために224*224のサイズにする\n",
    "      label = torch.as_tensor(labels[self.data.iloc[index]['label']])\n",
    "\n",
    "      return [x.to(device),label.to(device)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IAk31EenTYNi"
   },
   "outputs": [],
   "source": [
    "def myLoss(preds,target):\n",
    "    return nn.CrossEntropyLoss(weight=torch.as_tensor([1.,2.,4.]).to(device))(preds+1e-12,target)#コンペの評価指標が重み[1.,2.,4.]だったためこのように設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkNX4LNZTAV5"
   },
   "outputs": [],
   "source": [
    "import timm\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ViT, self).__init__()\n",
    "        # ここにViTのモデルアーキテクチャを実装\n",
    "        self.vit = timm.create_model('eva02_base_patch14_224', pretrained=True, num_classes=num_classes,in_chans=8,features_only=False,global_pool='avg')#timmからモデルを持ってくる\n",
    "        # self.model.conv1 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        # self.vit.head.drop.p = 0.5\n",
    "        #edgenext_base.in21k_ft_in1k\n",
    "        # self.vit.features.conv0=nn.Conv2d(5, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        # self.new_model=nn.Sequential(*self.layer)\n",
    "    def forward(self, x):\n",
    "        # ここに順伝播の処理を実装\n",
    "        return self.vit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0D2PU_MTTBPs"
   },
   "outputs": [],
   "source": [
    "from fastai.callback.core import Callback\n",
    "\n",
    "class SaveModelCallback(Callback):#モデルをエポックごとに保存するためのコールバック関数\n",
    "    def __init__(self, every_epoch=False, path='models', fname='model',with_opt=False):\n",
    "        self.every_epoch = every_epoch\n",
    "        self.path = path\n",
    "        self.fname = fname\n",
    "        self.with_opt = with_opt\n",
    "\n",
    "    def after_epoch(self):\n",
    "        # エポックごとにモデルを保存する\n",
    "        if self.every_epoch:\n",
    "            self.learn.save(f'{self.path}/{self.fname}_ep_{self.epoch}')\n",
    "\n",
    "save_model_cb = SaveModelCallback(with_opt=True,every_epoch=True, path='/content/drive/MyDrive/RSNA_csv', fname=f\"eva_p_90_ch_8_f_{fold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHtZMpKVOhph"
   },
   "outputs": [],
   "source": [
    "INF['EPOCHS']=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 95
    },
    "id": "pbVLpht1Tvgq",
    "outputId": "dc3ce0d8-8b57-4c98-c479-c245fec57bef"
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "if 1:\n",
    "    seed_everything(SEED)\n",
    "    with open('/content/drive/MyDrive/RSNA_csv/'+\"SEG_\"+\"1\"+\".pkl\", 'rb') as f:\n",
    "      UNet=pickle.load(f)\n",
    "    tds = ViT_T1_Dataset(tdf2,UNet,transform=transforms_train)#データセット関数からデータ拡張を適用して読み込む\n",
    "    vds = ViT_T1_Dataset(vdf2,UNet,VALID=True)#データセット関数から読み込む\n",
    "    tdl = torch.utils.data.DataLoader(tds, batch_size=16, shuffle=True, drop_last=True)#データローダーの作成\n",
    "    vdl = torch.utils.data.DataLoader(vds, batch_size=16, shuffle=False)#データローダーの作成\n",
    "\n",
    "    dls = DataLoaders(tdl,vdl)#fastai用のデータローダーの作成\n",
    "\n",
    "    n_iter = len(tds)//INF['BS']\n",
    "\n",
    "    model = ViT(num_classes=3)#モデル作成\n",
    "    model.to(device)\n",
    "    learn = Learner(#Learnerを作成\n",
    "        dls,\n",
    "        model,\n",
    "        lr=INF['LR'],\n",
    "        loss_func=myLoss,\n",
    "        cbs=[\n",
    "            save_model_cb,\n",
    "            GradientClip,\n",
    "            ShowGraphCallback(),#学習曲線を表示\n",
    "            # alpha_cb\n",
    "        ]\n",
    "    )\n",
    "    learn.fit_one_cycle(INF['EPOCHS'],wd=1e-2)\n",
    "    with open('/content/drive/MyDrive/RSNA_csv/'+\"VIT_\"+str(fold)+\".pkl\", 'wb') as f:\n",
    "      pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "CJmAtE91-7i8",
    "outputId": "e4c94f06-29bf-47c9-8366-60c9ef8923f9"
   },
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from torch.optim import AdamW\n",
    "import pickle\n",
    "\n",
    "if 1:\n",
    "    seed_everything(SEED)\n",
    "\n",
    "    # モデルの読み込み\n",
    "    with open('/content/drive/MyDrive/RSNA_csv/'+\"SEG_\"+\"1\"+\".pkl\", 'rb') as f:\n",
    "        UNet = pickle.load(f)\n",
    "\n",
    "    # Dataset 作成\n",
    "    tds = ViT_T1_Dataset(tdf2, UNet, transform=transforms_train)\n",
    "    vds = ViT_T1_Dataset(vdf2, UNet, VALID=True)\n",
    "    tdl = torch.utils.data.DataLoader(tds, batch_size=16, shuffle=True, drop_last=True)\n",
    "    vdl = torch.utils.data.DataLoader(vds, batch_size=16, shuffle=False)\n",
    "\n",
    "    # DataLoaders 設定\n",
    "    dls = DataLoaders(tdl, vdl)\n",
    "    n_iter = len(tds)//INF['BS']\n",
    "\n",
    "    # モデルと学習設定\n",
    "    model = ViT(num_classes=3)\n",
    "    model.to(device)\n",
    "\n",
    "    # Learner 作成\n",
    "    learn = Learner(\n",
    "        dls,\n",
    "        model,\n",
    "        lr=INF['LR'],\n",
    "        loss_func=myLoss,\n",
    "        cbs=[\n",
    "            save_model_cb,\n",
    "            GradientClip,\n",
    "            # ShowGraphCallback(),\n",
    "            # alpha_cb\n",
    "        ],# オプティマイザをAdamWに設定\n",
    "    )\n",
    "\n",
    "    # モデルを読み込む (with_opt=True でオプティマイザの状態も復元)\n",
    "    learn.load('/content/drive/MyDrive/RSNA_csv/eva_p_90_ch_8_f_1_ep_12', with_opt=True,device=device)\n",
    "    # Fine-tune モードで追加学習\n",
    "    learn.fit_one_cycle(40, start_epoch=13,wd=1e-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3eFO7qYeeih_"
   },
   "source": [
    "#混同行列の算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMnxtF1RgdvN"
   },
   "outputs": [],
   "source": [
    "vdf2=pd.read_pickle(\"/content/drive/MyDrive/RSNA_csv/grouped_df_sagt1_vdf2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 460,
     "status": "ok",
     "timestamp": 1736910305851,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "E5Ti69TN6w6J",
    "outputId": "b0c5f20c-166d-4e7e-9e66-a57c0c198397"
   },
   "outputs": [],
   "source": [
    "vdf2[['description','study_id','label','level']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJNgwgECzoH6"
   },
   "outputs": [],
   "source": [
    "ALL_CONDITIONS = sorted([\"left_neural_foraminal_narrowing\", \"right_neural_foraminal_narrowing\"])\n",
    "LEVELS = [\"l1_l2\", \"l2_l3\", \"l3_l4\", \"l4_l5\", \"l5_s1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gIY0QGv_ze_5"
   },
   "outputs": [],
   "source": [
    "# Pre-populate results df\n",
    "import glob\n",
    "import os\n",
    "study_ids = vdf2['study_id'].unique().tolist()\n",
    "\n",
    "results_df = pd.DataFrame({\"row_id\":[], \"normal_mild\": [], \"moderate\": [], \"severe\": []})\n",
    "for study_id in study_ids:\n",
    "    for condition in ALL_CONDITIONS:\n",
    "        for level in LEVELS:\n",
    "            row_id = f\"{study_id}_{condition}_{level}\"\n",
    "            results_df = results_df._append({\"row_id\": row_id, \"normal_mild\": 1/3, \"moderate\": 1/3, \"severe\": 1/3}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1736910377849,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "m3OEriQ97Ns9",
    "outputId": "6c97321a-64d7-49ee-afc5-b387b0df5761"
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D768j-YQxlYj"
   },
   "outputs": [],
   "source": [
    "grouped_df=vdf2\n",
    "with open(\"/content/drive/MyDrive/RSNA_csv/SEG_1.pkl\", 'rb') as f:\n",
    "  model1=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1QC6fbX2KFn6"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class ViT_T1_Dataset(Dataset):\n",
    "    def __init__(self, df, UNet, VALID=False, P=patch_size, alpha=0):\n",
    "        self.data = df\n",
    "        self.UNet = UNet\n",
    "        self.VALID = VALID\n",
    "        self.P = P\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = np.zeros((8, self.P, self.P), dtype=np.float32)\n",
    "        non_zero_slice=[]\n",
    "\n",
    "        for i in range(8):\n",
    "\n",
    "          sample = '/content/train_images/'\n",
    "          sample = sample+str(self.data.iloc[index]['study_id'])+'/'+str(self.data.iloc[index]['series_id'])+'/'+str(self.data.iloc[index][f'instance_number_{i}'])+'.dcm'\n",
    "\n",
    "          image = pydicom.dcmread(sample).pixel_array\n",
    "          H,W = image.shape\n",
    "          # centers = torch.as_tensor([x for x in row[coor]]).view(5,2).float()\n",
    "          # By plane resizing I've been distorting the proportions\n",
    "          if H > W:\n",
    "              d = W\n",
    "              h = (H - d)//2\n",
    "              image = image[h:h+d]\n",
    "              # centers[:,1] -= h\n",
    "              H = W\n",
    "          elif H < W:\n",
    "              d = H\n",
    "              w = (W - d)//2\n",
    "              image = image[:,w:w+d]\n",
    "              # centers[:,0] -= w\n",
    "              W = H\n",
    "          image = cv2.resize(image,(PATCH_SIZE,PATCH_SIZE))\n",
    "          image = torch.as_tensor(image/(np.max(image))).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "\n",
    "          # OUT = 0\n",
    "          # with torch.no_grad():\n",
    "                  # for rot in [0,1,2,3]:\n",
    "                          # OUT += torch.rot90(self.UNet(torch.rot90(image,rot,dims=[-2, -1])),-rot,dims=[-2, -1])\n",
    "          OUT=self.UNet(image)\n",
    "          OUT = (OUT > TH)[0]\n",
    "          c = (OUT.unsqueeze(1)*idx_map[0]).view(5,2,PATCH_SIZE*PATCH_SIZE).sum(-1)\n",
    "          d = OUT.view(5,PATCH_SIZE*PATCH_SIZE).sum(-1)\n",
    "          m = d > 0\n",
    "          c[m] = (c[m]/(d[m]).unsqueeze(-1)).long()\n",
    "          c[~m] = self.P # I have to find a better solution\n",
    "\n",
    "          image_slices = []\n",
    "\n",
    "          for xy in c:\n",
    "            y_start = max(0, xy[1] - self.P // 2)\n",
    "            y_end = min(512,xy[1] + self.P - self.P // 2)\n",
    "            x_start = max(0, xy[0] - self.P // 2)\n",
    "            x_end = min(512,xy[0] + self.P - self.P // 2)\n",
    "\n",
    "    # スライスが有効なサイズを持つか確認\n",
    "            if (y_end - y_start == self.P) and (x_end - x_start == self.P):\n",
    "              slice_img = image[0, 0, y_start:y_end, x_start:x_end]\n",
    "              non_zero_slice.append(slice_img)\n",
    "              image_slices.append(slice_img)\n",
    "            else:\n",
    "              zero_slice = torch.zeros((self.P, self.P), device=image.device)\n",
    "              image_slices.append(zero_slice)\n",
    "              # print(f\"Skipped slice due to incorrect size: {(y_end - y_start, x_end - x_start)}\")\n",
    "              # print(f\"Slice coordinates: {(xy[1], xy[0])}\")\n",
    "\n",
    "# スライスがあればスタックする\n",
    "          if image_slices:\n",
    "            try:\n",
    "              image = torch.stack(image_slices)\n",
    "            except RuntimeError as e:\n",
    "              print(f\"Error: {e}\")\n",
    "          else:\n",
    "            print(\"No valid slices available for stacking.\")\n",
    "\n",
    "          # if not self.VALID: image = augment_image(image,self.alpha)\n",
    "          x[i,...]=image[self.data.iloc[index]['level']].cpu().numpy()\n",
    "\n",
    "        for i in range(8):\n",
    "          if (x[i,...].sum() == 0) and (len(non_zero_slice)>0):\n",
    "            x[i,...] = non_zero_slice[0].cpu().numpy()\n",
    "          else:\n",
    "            pass\n",
    "          #nothing\n",
    "        x=torch.as_tensor(x).float()\n",
    "        x = F.interpolate(x.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n",
    "        # label = torch.as_tensor(labels[self.data.iloc[index]['label']])\n",
    "\n",
    "        return [x.to(device),m.to(device)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3703,
     "status": "ok",
     "timestamp": 1736914400385,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "upcnk3EFyO31",
    "outputId": "96cbace6-cc10-4f3f-8ce6-06ea6d26c84c"
   },
   "outputs": [],
   "source": [
    "model = ViT(num_classes=3)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "checkpoint = torch.load('/content/drive/MyDrive/RSNA_csv/eva_p_90_ch_8_f_1_ep_17.pth')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for i in range(8):  # Iterating from 0 to 4\n",
    "    column_name = f'instance_number_{i}'\n",
    "    grouped_df[column_name].fillna(1, inplace=True)  # Replacing NaN with 1\n",
    "\n",
    "\n",
    "test_s=ViT_T1_Dataset(grouped_df,model1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 958713,
     "status": "ok",
     "timestamp": 1736915758341,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "Z6OgNmKwehOG",
    "outputId": "2c92ed46-d0dc-419d-9341-3e5daaa820eb"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "grouped_df = grouped_df.reset_index(drop=True)\n",
    "\n",
    "study_id=grouped_df['study_id'].iloc[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_s)), desc=\"Processing predictions\"):\n",
    "      try:\n",
    "        row = grouped_df.iloc[i]\n",
    "        x=test_s.__getitem__(i)[0].unsqueeze(0)\n",
    "        probabilities = F.softmax(model(x), dim=1)\n",
    "      #   print(probabilities)\n",
    "        if row['description']=='Left Neural Foraminal Narrowing':\n",
    "          if row['level']==0:\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_left_neural_foraminal_narrowing_l1_l2\",'normal_mild']=probabilities[0][0].item()\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_left_neural_foraminal_narrowing_l1_l2\",'moderate']=probabilities[0][1].item()\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_left_neural_foraminal_narrowing_l1_l2\",'severe']=probabilities[0][2].item()\n",
    "          elif row['level']==1:\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_left_neural_foraminal_narrowing_l2_l3\",'normal_mild']=probabilities[0][0].item()\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_left_neural_foraminal_narrowing_l2_l3\",'moderate']=probabilities[0][1].item()\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_left_neural_foraminal_narrowing_l2_l3\",'severe']=probabilities[0][2].item()\n",
    "          elif row['level']==2:\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_left_neural_foraminal_narrowing_l3_l4\",'normal_mild']=probabilities[0][0].item()\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_left_neural_foraminal_narrowing_l3_l4\",'moderate']=probabilities[0][1].item()\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_left_neural_foraminal_narrowing_l3_l4\",'severe']=probabilities[0][2].item()\n",
    "          elif row['level']==3:\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_left_neural_foraminal_narrowing_l4_l5\",'normal_mild']=probabilities[0][0].item()\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_left_neural_foraminal_narrowing_l4_l5\",'moderate']=probabilities[0][1].item()\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_left_neural_foraminal_narrowing_l4_l5\",'severe']=probabilities[0][2].item()\n",
    "          elif row['level']==4:\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_left_neural_foraminal_narrowing_l5_s1\",'normal_mild']=probabilities[0][0].item()\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_left_neural_foraminal_narrowing_l5_s1\",'moderate']=probabilities[0][1].item()\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_left_neural_foraminal_narrowing_l5_s1\",'severe']=probabilities[0][2].item()\n",
    "        elif row['description']=='Right Neural Foraminal Narrowing':\n",
    "          if row['level']==0:\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_right_neural_foraminal_narrowing_l1_l2\",'normal_mild']=probabilities[0][0].item()\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_right_neural_foraminal_narrowing_l1_l2\",'moderate']=probabilities[0][1].item()\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_right_neural_foraminal_narrowing_l1_l2\",'severe']=probabilities[0][2].item()\n",
    "          elif row['level']==1:\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_right_neural_foraminal_narrowing_l2_l3\",'normal_mild']=probabilities[0][0].item()\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_right_neural_foraminal_narrowing_l2_l3\",'moderate']=probabilities[0][1].item()\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_right_neural_foraminal_narrowing_l2_l3\",'severe']=probabilities[0][2].item()\n",
    "          elif row['level']==2:\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_right_neural_foraminal_narrowing_l3_l4\",'normal_mild']=probabilities[0][0].item()\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_right_neural_foraminal_narrowing_l3_l4\",'moderate']=probabilities[0][1].item()\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_right_neural_foraminal_narrowing_l3_l4\",'severe']=probabilities[0][2].item()\n",
    "          elif row['level']==3:\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_right_neural_foraminal_narrowing_l4_l5\",'normal_mild']=probabilities[0][0].item()\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_right_neural_foraminal_narrowing_l4_l5\",'moderate']=probabilities[0][1].item()\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_right_neural_foraminal_narrowing_l4_l5\",'severe']=probabilities[0][2].item()\n",
    "          elif row['level']==4:\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_right_neural_foraminal_narrowing_l5_s1\",'normal_mild']=probabilities[0][0].item()\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_right_neural_foraminal_narrowing_l5_s1\",'moderate']=probabilities[0][1].item()\n",
    "            results_df.loc[results_df['row_id']==f\"{row['study_id']}_right_neural_foraminal_narrowing_l5_s1\",'severe']=probabilities[0][2].item()\n",
    "      except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1736915838494,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "lJXDbjTSQCdW",
    "outputId": "6531a287-91ab-49dc-9bd5-4edad563bc6c"
   },
   "outputs": [],
   "source": [
    "vdf2['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6645,
     "status": "ok",
     "timestamp": 1736915931751,
     "user": {
      "displayName": "藤吉亨",
      "userId": "10408607038785028470"
     },
     "user_tz": -540
    },
    "id": "Sg8EgBSTNwnz",
    "outputId": "8bc8c560-83ec-4d5f-9970-eb0d147bde29"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# マッピングのための辞書を用意\n",
    "description_to_condition = {\n",
    "    \"Left Neural Foraminal Narrowing\": \"left_neural_foraminal_narrowing\",\n",
    "    \"Right Neural Foraminal Narrowing\": \"right_neural_foraminal_narrowing\",\n",
    "}\n",
    "\n",
    "level_to_level_t = {\n",
    "    0: \"l1_l2\",\n",
    "    1: \"l2_l3\",\n",
    "    2: \"l3_l4\",\n",
    "    3: \"l4_l5\",\n",
    "    4: \"l5_s1\",\n",
    "}\n",
    "\n",
    "true_label_to_true_label_t={\n",
    "    \"Normal/Mild\":\"normal_mild\",\n",
    "    \"Moderate\":\"moderate\",\n",
    "    \"Severe\":\"severe\"\n",
    "}\n",
    "\n",
    "# vdf2 の正解データを results_df にマッピング\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for _, row in vdf2.iterrows():\n",
    "    study_id = row['study_id']\n",
    "    description = row['description']\n",
    "    level = row['level']\n",
    "    level_t=level_to_level_t[level]\n",
    "    condition = description_to_condition[description]\n",
    "\n",
    "\n",
    "    # 正解ラベル\n",
    "    true_label = row['label']  # 正解ラベルがここにあると仮定\n",
    "    true_label_t=true_label_to_true_label_t[true_label]\n",
    "    true_labels.append(true_label_t)\n",
    "\n",
    "    # 予測確率に基づいて予測ラベルを取得\n",
    "    row_id = f\"{study_id}_{condition}_{level_t}\"\n",
    "    predicted_row = results_df[results_df['row_id'] == row_id]\n",
    "\n",
    "    if not predicted_row.empty:\n",
    "        # 最大確率の列名が予測ラベル\n",
    "        predicted_label = predicted_row[['normal_mild', 'moderate', 'severe']].idxmax(axis=1).values[0]\n",
    "        predicted_labels.append(predicted_label)\n",
    "    else:\n",
    "        print(f\"Warning: No prediction found for row_id {row_id}\")\n",
    "        predicted_labels.append(None)  # 空の場合は None を設定\n",
    "\n",
    "# None を除外\n",
    "true_labels_filtered = [t for t, p in zip(true_labels, predicted_labels) if p is not None]\n",
    "predicted_labels_filtered = [p for p in predicted_labels if p is not None]\n",
    "\n",
    "# 混同行列を計算\n",
    "conf_matrix = confusion_matrix(true_labels_filtered, predicted_labels_filtered, labels=[\"normal_mild\", \"moderate\", \"severe\"])\n",
    "\n",
    "# 結果表示\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# 詳細レポート\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels_filtered, predicted_labels_filtered, target_names=[\"normal_mild\", \"moderate\", \"severe\"]))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPVvhw3jNfu1h4rdbA/NKY6",
   "gpuType": "T4",
   "mount_file_id": "1a5TCJGTHWbpO9yOjUUHgO1kvgoTbq7sU",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
